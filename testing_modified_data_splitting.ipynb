{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55c821b6-2777-44ef-9fb4-f9002f4a2df0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-17 10:43:47.592323: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-08-17 10:43:48.784827: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/lib/x86_64-linux-gnu/:/opt/conda/lib\n",
      "2025-08-17 10:43:48.785021: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/lib/x86_64-linux-gnu/:/opt/conda/lib\n",
      "2025-08-17 10:43:48.785032: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/opt/conda/envs/flwr/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/conda/envs/flwr/lib/python3.10/site-packages/ray/_private/parameter.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n",
      "2025-08-17 10:43:49,905\tINFO util.py:159 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os, sys\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "from dataloader.utils import task_splitter_circle_arrow, task_splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "91598efe-2b2c-4784-a698-26aea4e0d235",
   "metadata": {},
   "outputs": [],
   "source": [
    "clients, aug, batch_size = (7,True,16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0d2aa995-7a2f-411f-b777-739adeb1bfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_task_cl, incremental_test_loaders, all_test_loader, y_labels, raw_df = task_splitter('data', clients, aug, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "60b7e42d-3416-4f1c-b385-ea5e1c0a0041",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_task_cl, incremental_test_loaders, all_test_loader, y_labels, raw_df = task_splitter_circle_arrow('Data', clients, aug, batch_size)\n",
    "raw_df = raw_df.drop('Using circle', axis='columns')\n",
    "raw_df = raw_df.rename(columns={'Using arrow':'domain'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "37922233-a243-4eb7-908a-a764415a435a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows in raw_df: 1600\n",
      "\n",
      "Raw dataframe counts per domain:\n",
      "Home           800\n",
      "BigOffice-2    160\n",
      "BigOffice-3    160\n",
      "Hallway        160\n",
      "MeetingRoom    160\n",
      "SmallOffice    160\n",
      "Name: domain, dtype: int64\n",
      "\n",
      "Task 0:\n",
      "  Client 0: Loader dataset size = 800\n",
      "  Client 1: Loader dataset size = 800\n",
      "  Client 2: Loader dataset size = 800\n",
      "  Client 3: Loader dataset size = 800\n",
      "  Client 4: Loader dataset size = 800\n",
      "  Client 5: Loader dataset size = 800\n",
      "  Client 6: Loader dataset size = 800\n",
      "  Client 7: Loader dataset size = 800\n",
      "  Client 8: Loader dataset size = 800\n",
      "  Client 9: Loader dataset size = 800\n",
      "  Total samples in split loaders = 8000\n",
      "  Raw dataframe count for domain 'Home' = 800\n",
      "  ✓ Split is augmented. Augmentation factor: 10\n",
      "\n",
      "Task 1:\n",
      "  Client 0: Loader dataset size = 160\n",
      "  Client 1: Loader dataset size = 160\n",
      "  Client 2: Loader dataset size = 160\n",
      "  Client 3: Loader dataset size = 160\n",
      "  Client 4: Loader dataset size = 160\n",
      "  Client 5: Loader dataset size = 160\n",
      "  Client 6: Loader dataset size = 160\n",
      "  Client 7: Loader dataset size = 160\n",
      "  Client 8: Loader dataset size = 160\n",
      "  Client 9: Loader dataset size = 160\n",
      "  Total samples in split loaders = 1600\n",
      "  Raw dataframe count for domain 'BigOffice-2' = 160\n",
      "  ✓ Split is augmented. Augmentation factor: 10\n",
      "\n",
      "Task 2:\n",
      "  Client 0: Loader dataset size = 160\n",
      "  Client 1: Loader dataset size = 160\n",
      "  Client 2: Loader dataset size = 160\n",
      "  Client 3: Loader dataset size = 160\n",
      "  Client 4: Loader dataset size = 160\n",
      "  Client 5: Loader dataset size = 160\n",
      "  Client 6: Loader dataset size = 160\n",
      "  Client 7: Loader dataset size = 160\n",
      "  Client 8: Loader dataset size = 160\n",
      "  Client 9: Loader dataset size = 160\n",
      "  Total samples in split loaders = 1600\n",
      "  Raw dataframe count for domain 'BigOffice-3' = 160\n",
      "  ✓ Split is augmented. Augmentation factor: 10\n",
      "\n",
      "Task 3:\n",
      "  Client 0: Loader dataset size = 160\n",
      "  Client 1: Loader dataset size = 160\n",
      "  Client 2: Loader dataset size = 160\n",
      "  Client 3: Loader dataset size = 160\n",
      "  Client 4: Loader dataset size = 160\n",
      "  Client 5: Loader dataset size = 160\n",
      "  Client 6: Loader dataset size = 160\n",
      "  Client 7: Loader dataset size = 160\n",
      "  Client 8: Loader dataset size = 160\n",
      "  Client 9: Loader dataset size = 160\n",
      "  Total samples in split loaders = 1600\n",
      "  Raw dataframe count for domain 'Hallway' = 160\n",
      "  ✓ Split is augmented. Augmentation factor: 10\n",
      "\n",
      "Task 4:\n",
      "  Client 0: Loader dataset size = 160\n",
      "  Client 1: Loader dataset size = 160\n",
      "  Client 2: Loader dataset size = 160\n",
      "  Client 3: Loader dataset size = 160\n",
      "  Client 4: Loader dataset size = 160\n",
      "  Client 5: Loader dataset size = 160\n",
      "  Client 6: Loader dataset size = 160\n",
      "  Client 7: Loader dataset size = 160\n",
      "  Client 8: Loader dataset size = 160\n",
      "  Client 9: Loader dataset size = 160\n",
      "  Total samples in split loaders = 1600\n",
      "  Raw dataframe count for domain 'MeetingRoom' = 160\n",
      "  ✓ Split is augmented. Augmentation factor: 10\n",
      "\n",
      "Task 5:\n",
      "  Client 0: Loader dataset size = 160\n",
      "  Client 1: Loader dataset size = 160\n",
      "  Client 2: Loader dataset size = 160\n",
      "  Client 3: Loader dataset size = 160\n",
      "  Client 4: Loader dataset size = 160\n",
      "  Client 5: Loader dataset size = 160\n",
      "  Client 6: Loader dataset size = 160\n",
      "  Client 7: Loader dataset size = 160\n",
      "  Client 8: Loader dataset size = 160\n",
      "  Client 9: Loader dataset size = 160\n",
      "  Total samples in split loaders = 1600\n",
      "  Raw dataframe count for domain 'SmallOffice' = 160\n",
      "  ✓ Split is augmented. Augmentation factor: 10\n",
      "\n",
      "Total samples across all tasks/clients: 16000\n",
      "Total rows in raw_df: 1600\n",
      "Overall split is larger than raw data (augmentation/duplication)\n",
      "\n",
      "Task 0, Client 0: Num batches = 50, Batch size = 16\n",
      "Task 0, Client 1: Num batches = 50, Batch size = 16\n",
      "Task 0, Client 2: Num batches = 50, Batch size = 16\n",
      "Task 0, Client 3: Num batches = 50, Batch size = 16\n",
      "Task 0, Client 4: Num batches = 50, Batch size = 16\n",
      "Task 0, Client 5: Num batches = 50, Batch size = 16\n",
      "Task 0, Client 6: Num batches = 50, Batch size = 16\n",
      "Task 0, Client 7: Num batches = 50, Batch size = 16\n",
      "Task 0, Client 8: Num batches = 50, Batch size = 16\n",
      "Task 0, Client 9: Num batches = 50, Batch size = 16\n",
      "\n",
      "Task 1, Client 0: Num batches = 10, Batch size = 16\n",
      "Task 1, Client 1: Num batches = 10, Batch size = 16\n",
      "Task 1, Client 2: Num batches = 10, Batch size = 16\n",
      "Task 1, Client 3: Num batches = 10, Batch size = 16\n",
      "Task 1, Client 4: Num batches = 10, Batch size = 16\n",
      "Task 1, Client 5: Num batches = 10, Batch size = 16\n",
      "Task 1, Client 6: Num batches = 10, Batch size = 16\n",
      "Task 1, Client 7: Num batches = 10, Batch size = 16\n",
      "Task 1, Client 8: Num batches = 10, Batch size = 16\n",
      "Task 1, Client 9: Num batches = 10, Batch size = 16\n",
      "\n",
      "Task 2, Client 0: Num batches = 10, Batch size = 16\n",
      "Task 2, Client 1: Num batches = 10, Batch size = 16\n",
      "Task 2, Client 2: Num batches = 10, Batch size = 16\n",
      "Task 2, Client 3: Num batches = 10, Batch size = 16\n",
      "Task 2, Client 4: Num batches = 10, Batch size = 16\n",
      "Task 2, Client 5: Num batches = 10, Batch size = 16\n",
      "Task 2, Client 6: Num batches = 10, Batch size = 16\n",
      "Task 2, Client 7: Num batches = 10, Batch size = 16\n",
      "Task 2, Client 8: Num batches = 10, Batch size = 16\n",
      "Task 2, Client 9: Num batches = 10, Batch size = 16\n",
      "\n",
      "Task 3, Client 0: Num batches = 10, Batch size = 16\n",
      "Task 3, Client 1: Num batches = 10, Batch size = 16\n",
      "Task 3, Client 2: Num batches = 10, Batch size = 16\n",
      "Task 3, Client 3: Num batches = 10, Batch size = 16\n",
      "Task 3, Client 4: Num batches = 10, Batch size = 16\n",
      "Task 3, Client 5: Num batches = 10, Batch size = 16\n",
      "Task 3, Client 6: Num batches = 10, Batch size = 16\n",
      "Task 3, Client 7: Num batches = 10, Batch size = 16\n",
      "Task 3, Client 8: Num batches = 10, Batch size = 16\n",
      "Task 3, Client 9: Num batches = 10, Batch size = 16\n",
      "\n",
      "Task 4, Client 0: Num batches = 10, Batch size = 16\n",
      "Task 4, Client 1: Num batches = 10, Batch size = 16\n",
      "Task 4, Client 2: Num batches = 10, Batch size = 16\n",
      "Task 4, Client 3: Num batches = 10, Batch size = 16\n",
      "Task 4, Client 4: Num batches = 10, Batch size = 16\n",
      "Task 4, Client 5: Num batches = 10, Batch size = 16\n",
      "Task 4, Client 6: Num batches = 10, Batch size = 16\n",
      "Task 4, Client 7: Num batches = 10, Batch size = 16\n",
      "Task 4, Client 8: Num batches = 10, Batch size = 16\n",
      "Task 4, Client 9: Num batches = 10, Batch size = 16\n",
      "\n",
      "Task 5, Client 0: Num batches = 10, Batch size = 16\n",
      "Task 5, Client 1: Num batches = 10, Batch size = 16\n",
      "Task 5, Client 2: Num batches = 10, Batch size = 16\n",
      "Task 5, Client 3: Num batches = 10, Batch size = 16\n",
      "Task 5, Client 4: Num batches = 10, Batch size = 16\n",
      "Task 5, Client 5: Num batches = 10, Batch size = 16\n",
      "Task 5, Client 6: Num batches = 10, Batch size = 16\n",
      "Task 5, Client 7: Num batches = 10, Batch size = 16\n",
      "Task 5, Client 8: Num batches = 10, Batch size = 16\n",
      "Task 5, Client 9: Num batches = 10, Batch size = 16\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Total rows in raw_df:\", len(raw_df))\n",
    "\n",
    "# 1. Task splits by 'domain'\n",
    "print(\"\\nRaw dataframe counts per domain:\")\n",
    "print(raw_df['domain'].value_counts())\n",
    "\n",
    "# 2. Count total number of samples actually present in each task/client split\n",
    "for task_idx, task_loaders in enumerate(train_task_cl):\n",
    "    total_split_samples = 0\n",
    "    print(f\"\\nTask {task_idx}:\")\n",
    "    for client_idx, loader in enumerate(task_loaders):\n",
    "        ds_len = len(loader.dataset)\n",
    "        total_split_samples += ds_len\n",
    "        print(f\"  Client {client_idx}: Loader dataset size = {ds_len}\")\n",
    "\n",
    "    # Compare to corresponding raw domain count (after augmentation, if any)\n",
    "    domain_name = raw_df['domain'].unique()[task_idx]\n",
    "    true_raw_count = raw_df[raw_df['domain'] == domain_name].shape[0]\n",
    "    print(f\"  Total samples in split loaders = {total_split_samples}\")\n",
    "    print(f\"  Raw dataframe count for domain '{domain_name}' = {true_raw_count}\")\n",
    "\n",
    "    # If augmenting, you should expect total_split_samples == true_raw_count * N_AUG\n",
    "    # Otherwise, expect equality.\n",
    "\n",
    "    if total_split_samples == true_raw_count:\n",
    "        print(\"  ✓ Split count matches raw count.\")\n",
    "    elif total_split_samples % true_raw_count == 0:\n",
    "        aug_factor = total_split_samples // true_raw_count\n",
    "        print(f\"  ✓ Split is augmented. Augmentation factor: {aug_factor}\")\n",
    "    else:\n",
    "        print(\"  ⚠ Warning: Split count ({}) does not match or scale cleanly from raw count ({}) !\"\n",
    "              .format(total_split_samples, true_raw_count))\n",
    "\n",
    "# 3. Global check: sum all split samples vs. entire raw dataframe\n",
    "split_total = sum(len(loader.dataset) for task_loaders in train_task_cl for loader in task_loaders)\n",
    "print(\"\\nTotal samples across all tasks/clients:\", split_total)\n",
    "print(\"Total rows in raw_df:\", len(raw_df))\n",
    "if split_total == len(raw_df):\n",
    "    print(\"Overall split matches raw data rows\")\n",
    "elif split_total > len(raw_df):\n",
    "    print(\"Overall split is larger than raw data (augmentation/duplication)\")\n",
    "else:\n",
    "    print(\"Overall split is smaller than raw data! Potential data loss.\")\n",
    "\n",
    "# 4. You can also print the DataLoader batch counts in each split\n",
    "for task_idx, task_loaders in enumerate(train_task_cl):\n",
    "    print('')\n",
    "    for client_idx, loader in enumerate(task_loaders):\n",
    "        print(f\"Task {task_idx}, Client {client_idx}: Num batches = {len(loader)}, Batch size = {loader.batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b597a30e-d1b9-4529-be44-f32d439c1c2f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train tasks: 6\n",
      "Task 0:\n",
      "  Number of clients: 7\n",
      "    Client 0: Dataset size = 1142, Number of batches = 72\n",
      "    Client 1: Dataset size = 1142, Number of batches = 72\n",
      "    Client 2: Dataset size = 1142, Number of batches = 72\n",
      "    Client 3: Dataset size = 1142, Number of batches = 72\n",
      "    Client 4: Dataset size = 1142, Number of batches = 72\n",
      "    Client 5: Dataset size = 1142, Number of batches = 72\n",
      "    Client 6: Dataset size = 1148, Number of batches = 72\n",
      "  Total samples for task 0: 8000\n",
      "Task 1:\n",
      "  Number of clients: 7\n",
      "    Client 0: Dataset size = 228, Number of batches = 15\n",
      "    Client 1: Dataset size = 228, Number of batches = 15\n",
      "    Client 2: Dataset size = 228, Number of batches = 15\n",
      "    Client 3: Dataset size = 228, Number of batches = 15\n",
      "    Client 4: Dataset size = 228, Number of batches = 15\n",
      "    Client 5: Dataset size = 228, Number of batches = 15\n",
      "    Client 6: Dataset size = 232, Number of batches = 15\n",
      "  Total samples for task 1: 1600\n",
      "Task 2:\n",
      "  Number of clients: 7\n",
      "    Client 0: Dataset size = 228, Number of batches = 15\n",
      "    Client 1: Dataset size = 228, Number of batches = 15\n",
      "    Client 2: Dataset size = 228, Number of batches = 15\n",
      "    Client 3: Dataset size = 228, Number of batches = 15\n",
      "    Client 4: Dataset size = 228, Number of batches = 15\n",
      "    Client 5: Dataset size = 228, Number of batches = 15\n",
      "    Client 6: Dataset size = 232, Number of batches = 15\n",
      "  Total samples for task 2: 1600\n",
      "Task 3:\n",
      "  Number of clients: 7\n",
      "    Client 0: Dataset size = 228, Number of batches = 15\n",
      "    Client 1: Dataset size = 228, Number of batches = 15\n",
      "    Client 2: Dataset size = 228, Number of batches = 15\n",
      "    Client 3: Dataset size = 228, Number of batches = 15\n",
      "    Client 4: Dataset size = 228, Number of batches = 15\n",
      "    Client 5: Dataset size = 228, Number of batches = 15\n",
      "    Client 6: Dataset size = 232, Number of batches = 15\n",
      "  Total samples for task 3: 1600\n",
      "Task 4:\n",
      "  Number of clients: 7\n",
      "    Client 0: Dataset size = 228, Number of batches = 15\n",
      "    Client 1: Dataset size = 228, Number of batches = 15\n",
      "    Client 2: Dataset size = 228, Number of batches = 15\n",
      "    Client 3: Dataset size = 228, Number of batches = 15\n",
      "    Client 4: Dataset size = 228, Number of batches = 15\n",
      "    Client 5: Dataset size = 228, Number of batches = 15\n",
      "    Client 6: Dataset size = 232, Number of batches = 15\n",
      "  Total samples for task 4: 1600\n",
      "Task 5:\n",
      "  Number of clients: 7\n",
      "    Client 0: Dataset size = 228, Number of batches = 15\n",
      "    Client 1: Dataset size = 228, Number of batches = 15\n",
      "    Client 2: Dataset size = 228, Number of batches = 15\n",
      "    Client 3: Dataset size = 228, Number of batches = 15\n",
      "    Client 4: Dataset size = 228, Number of batches = 15\n",
      "    Client 5: Dataset size = 228, Number of batches = 15\n",
      "    Client 6: Dataset size = 232, Number of batches = 15\n",
      "  Total samples for task 5: 1600\n",
      "\n",
      "Incremental Test Loaders:\n",
      "  Test loader 0: Dataset size = 40, Batches = 3\n",
      "  Test loader 1: Dataset size = 80, Batches = 5\n",
      "  Test loader 2: Dataset size = 120, Batches = 8\n",
      "  Test loader 3: Dataset size = 160, Batches = 10\n",
      "  Test loader 4: Dataset size = 200, Batches = 13\n",
      "  Test loader 5: Dataset size = 240, Batches = 15\n",
      "\n",
      "All Test Loader:\n",
      "  Dataset size = 240, Batches = 15\n",
      "\n",
      "Label columns: Index(['Vaccum Cleaning', 'Mopping the Floor', 'Carry Warm Food',\n",
      "       'Carry Cold Food', 'Carry Drinks', 'Carry Small Objects',\n",
      "       'Carry Large Objects', 'Cleaning', 'Starting a conversation'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of train tasks:\", len(train_task_cl))\n",
    "for task_idx, task_loaders in enumerate(train_task_cl):\n",
    "    print(f\"Task {task_idx}:\")\n",
    "    print(\"  Number of clients:\", len(task_loaders))\n",
    "    for client_idx, loader in enumerate(task_loaders):\n",
    "        print(f\"    Client {client_idx}: Dataset size = {len(loader.dataset)}, Number of batches = {len(loader)}\")\n",
    "    total_samples = sum([len(loader.dataset) for loader in task_loaders])\n",
    "    print(f\"  Total samples for task {task_idx}: {total_samples}\")\n",
    "\n",
    "print(\"\\nIncremental Test Loaders:\")\n",
    "for i, loader in enumerate(incremental_test_loaders):\n",
    "    print(f\"  Test loader {i}: Dataset size = {len(loader.dataset)}, Batches = {len(loader)}\")\n",
    "\n",
    "print(\"\\nAll Test Loader:\")\n",
    "print(f\"  Dataset size = {len(all_test_loader.dataset)}, Batches = {len(all_test_loader)}\")\n",
    "\n",
    "print(\"\\nLabel columns:\", y_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6f9136-b6fb-4d26-a85a-8bf035298afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "Number of train tasks: 2\n",
    "Task 0:\n",
    "  Number of clients: 10\n",
    "    Client 0: Dataset size = 294, Number of batches = 18\n",
    "    Client 1: Dataset size = 294, Number of batches = 18\n",
    "    Client 2: Dataset size = 294, Number of batches = 18\n",
    "    Client 3: Dataset size = 294, Number of batches = 18\n",
    "    Client 4: Dataset size = 294, Number of batches = 18\n",
    "    Client 5: Dataset size = 294, Number of batches = 18\n",
    "    Client 6: Dataset size = 294, Number of batches = 18\n",
    "    Client 7: Dataset size = 294, Number of batches = 18\n",
    "    Client 8: Dataset size = 294, Number of batches = 18\n",
    "    Client 9: Dataset size = 294, Number of batches = 18\n",
    "  Total samples for task 0: 2940\n",
    "Task 1:\n",
    "  Number of clients: 10\n",
    "    Client 0: Dataset size = 267, Number of batches = 16\n",
    "    Client 1: Dataset size = 267, Number of batches = 16\n",
    "    Client 2: Dataset size = 267, Number of batches = 16\n",
    "    Client 3: Dataset size = 267, Number of batches = 16\n",
    "    Client 4: Dataset size = 267, Number of batches = 16\n",
    "    Client 5: Dataset size = 267, Number of batches = 16\n",
    "    Client 6: Dataset size = 267, Number of batches = 16\n",
    "    Client 7: Dataset size = 267, Number of batches = 16\n",
    "    Client 8: Dataset size = 267, Number of batches = 16\n",
    "    Client 9: Dataset size = 267, Number of batches = 16\n",
    "  Total samples for task 1: 2670\n",
    "\n",
    "Incremental Test Loaders:\n",
    "  Test loader 0: Dataset size = 99, Batches = 6\n",
    "  Test loader 1: Dataset size = 90, Batches = 5\n",
    "\n",
    "All Test Loader:\n",
    "  Dataset size = 189, Batches = 11"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "flwr",
   "name": "workbench-notebooks.m131",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m131"
  },
  "kernelspec": {
   "display_name": "Python (flwr) (Local)",
   "language": "python",
   "name": "flwr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
